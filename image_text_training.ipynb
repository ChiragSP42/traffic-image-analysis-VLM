{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abc93ccb-a4c6-43f7-9211-ef1a0af1aa10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T19:50:05.616787Z",
     "iopub.status.busy": "2025-09-03T19:50:05.616639Z",
     "iopub.status.idle": "2025-09-03T19:50:10.600709Z",
     "shell.execute_reply": "2025-09-03T19:50:10.600068Z",
     "shell.execute_reply.started": "2025-09-03T19:50:05.616765Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List, Any, Optional\n",
    "import logging\n",
    "from io import BytesIO\n",
    "import torch\n",
    "import json\n",
    "from aws_helpers import helpers\n",
    "from transformers import AutoTokenizer, AutoModelForImageTextToText\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "516a2ac9-b0da-46ed-aa32-d85ad0eff178",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T19:50:10.601540Z",
     "iopub.status.busy": "2025-09-03T19:50:10.601263Z",
     "iopub.status.idle": "2025-09-03T19:50:19.183562Z",
     "shell.execute_reply": "2025-09-03T19:50:19.183017Z",
     "shell.execute_reply.started": "2025-09-03T19:50:10.601523Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-03 19:50:11.468151: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756929011.480377    2280 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756929011.484075    2280 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-09-03 19:50:11.496375: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dd3bce25f5b4020a87000e6db0dfdb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2587393314.py:main:213 - INFO - Creating dataset\n",
      "2587393314.py:main:215 - INFO - Created dataset with 16 samples\n",
      "2587393314.py:main:217 - INFO - Creating collator\n",
      "2587393314.py:main:220 - INFO - Creating dataloader\n",
      "2587393314.py:main:239 - INFO - Starting training...\n",
      "2587393314.py:main:259 - ERROR - ❌ Step 0 failed: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 21.98 GiB of which 10.44 MiB is free. Process 7595 has 21.96 GiB memory in use. Of the allocated memory 21.11 GiB is allocated by PyTorch, and 556.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "logger = helpers._setup_logger(level=logging.DEBUG)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_ID = 'OpenGVLab/InternVL3-8B-hf'\n",
    "S3_BUCKET = 'signal-8-flock'\n",
    "\n",
    "def create_template(json_obj: Dict) -> str:\n",
    "    template = \"\"\n",
    "    if json_obj[\"year\"] != \"None\":\n",
    "        template += json_obj['year'] + \" \"\n",
    "    if json_obj[\"car_type\"] != \"None\":\n",
    "        template += json_obj[\"car_type\"] + \" \"\n",
    "    if json_obj[\"color\"] != \"None\":\n",
    "        template += json_obj['color'] + \" \"\n",
    "    if json_obj[\"make\"] != \"None\":\n",
    "        template += json_obj[\"make\"] + \" \"\n",
    "    if json_obj[\"model\"] != \"None\":\n",
    "        template += json_obj[\"model\"] + \" \"\n",
    "    if json_obj[\"license_plate\"] != \"None\":\n",
    "        template += f\"with license plate number {json_obj['license_plate']} \"\n",
    "    if json_obj['unique_identifiers']:\n",
    "        template += f\"has the following unique identifiers: \" + \", \".join(json_obj[\"unique_identifiers\"])\n",
    "    return template\n",
    "\n",
    "def load_image_from_s3(s3_uri: str) -> Image.Image:\n",
    "    s3_client = helpers._get_s3_client()\n",
    "    path = s3_uri[5:]\n",
    "    bucket, key = path.split(\"/\", 1)\n",
    "    obj = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    img_bytes = obj[\"Body\"].read()\n",
    "    img = Image.open(BytesIO(img_bytes)).convert(\"RGB\")\n",
    "    return img\n",
    "\n",
    "# Manual image processing function\n",
    "def process_image(image: Image.Image, image_size: int = 448):\n",
    "    \"\"\"\n",
    "    Manually process image without AutoProcessor\n",
    "    InternVL uses 448x448 base resolution with specific normalization\n",
    "    \"\"\"\n",
    "    transform = T.Compose([\n",
    "        T.Resize((image_size, image_size)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],  # ImageNet normalization\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    ])\n",
    "    return transform(image)\n",
    "\n",
    "def create_conversation_text(text: str) -> tuple[str, str, str]:\n",
    "    \"\"\"\n",
    "    Create conversation without special tokens - just plain text\n",
    "    Returns: (full_conversation, user_part, assistant_part)\n",
    "    \"\"\"\n",
    "    with open(\"user_prompt.txt\", 'r') as f:\n",
    "        user_prompt = f.read()\n",
    "    assistant_response = text\n",
    "    \n",
    "    # Create a simple conversation format\n",
    "    conversation = f\"User: {user_prompt}\\nAssistant: {assistant_response}\"\n",
    "    \n",
    "    return conversation, user_prompt, assistant_response\n",
    "\n",
    "def tokenize_conversation(tokenizer, conversation: str, user_prompt: str, assistant_response: str, max_length: int = 512):\n",
    "    \"\"\"\n",
    "    Manually tokenize conversation and create labels\n",
    "    Only assistant tokens contribute to loss\n",
    "    \"\"\"\n",
    "    # Tokenize full conversation\n",
    "    full_tokens = tokenizer.encode(conversation, add_special_tokens=True, max_length=max_length, truncation=True)\n",
    "    \n",
    "    # Find where assistant response starts\n",
    "    user_part = f\"User: {user_prompt}\\nAssistant: \"\n",
    "    user_tokens = tokenizer.encode(user_part, add_special_tokens=False)\n",
    "    \n",
    "    # Create labels - mask non-assistant tokens with -100\n",
    "    labels = [-100] * len(full_tokens)\n",
    "    \n",
    "    # Find assistant start position\n",
    "    assistant_start = len(user_tokens)\n",
    "    if assistant_start < len(full_tokens):\n",
    "        # Only apply loss to assistant response tokens\n",
    "        for i in range(assistant_start, len(full_tokens)):\n",
    "            labels[i] = full_tokens[i]\n",
    "    \n",
    "    return {\n",
    "        'input_ids': torch.tensor(full_tokens),\n",
    "        'labels': torch.tensor(labels),\n",
    "        'attention_mask': torch.ones(len(full_tokens))\n",
    "    }\n",
    "\n",
    "class ManualImageTextDataset(Dataset):\n",
    "    def __init__(self, s3_bucket: str, json_file: str):\n",
    "        self.records = []\n",
    "        s3_client = helpers._get_s3_client()\n",
    "        input_json_file = s3_client.get_object(Bucket=s3_bucket, Key=json_file)[\"Body\"].read().decode('utf-8')\n",
    "        input_json_file = json.loads(input_json_file)\n",
    "        \n",
    "        for json_obj in input_json_file['output'][:16]:\n",
    "            template = create_template(json_obj)\n",
    "            image_uri = json_obj[\"s3_uri\"]\n",
    "            self.records.append({\"image_uri\": image_uri, \"text\": template})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        record = self.records[idx]\n",
    "        \n",
    "        # Load and process image manually\n",
    "        image = load_image_from_s3(s3_uri=record[\"image_uri\"])\n",
    "        pixel_values = process_image(image)\n",
    "        \n",
    "        # Create conversation text\n",
    "        conversation, user_prompt, assistant_response = create_conversation_text(record[\"text\"])\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": pixel_values.to(torch.bfloat16),\n",
    "            \"conversation\": conversation,\n",
    "            \"user_prompt\": user_prompt,\n",
    "            \"assistant_response\": assistant_response\n",
    "        }\n",
    "\n",
    "class ManualCollator:\n",
    "    def __init__(self, tokenizer, max_length: int = 512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        # Stack image tensors\n",
    "        pixel_values = torch.stack([item['pixel_values'] for item in batch])\n",
    "        \n",
    "        # Tokenize conversations\n",
    "        tokenized_batch = []\n",
    "        for item in batch:\n",
    "            tokenized = tokenize_conversation(\n",
    "                self.tokenizer, \n",
    "                item['conversation'], \n",
    "                item['user_prompt'], \n",
    "                item['assistant_response'],\n",
    "                self.max_length\n",
    "            )\n",
    "            tokenized_batch.append(tokenized)\n",
    "        \n",
    "        # Pad sequences to same length\n",
    "        max_len = max(len(item['input_ids']) for item in tokenized_batch)\n",
    "        \n",
    "        batch_input_ids = []\n",
    "        batch_labels = []\n",
    "        batch_attention_mask = []\n",
    "        \n",
    "        for item in tokenized_batch:\n",
    "            input_ids = item['input_ids']\n",
    "            labels = item['labels']\n",
    "            attention_mask = item['attention_mask']\n",
    "            \n",
    "            # Pad sequences\n",
    "            pad_len = max_len - len(input_ids)\n",
    "            \n",
    "            padded_input_ids = F.pad(input_ids, (0, pad_len), value=self.tokenizer.pad_token_id)\n",
    "            padded_labels = F.pad(labels, (0, pad_len), value=-100)\n",
    "            padded_attention = F.pad(attention_mask, (0, pad_len), value=0)\n",
    "            \n",
    "            batch_input_ids.append(padded_input_ids)\n",
    "            batch_labels.append(padded_labels)\n",
    "            batch_attention_mask.append(padded_attention)\n",
    "        \n",
    "        return {\n",
    "            'pixel_values': pixel_values,\n",
    "            'input_ids': torch.stack(batch_input_ids),\n",
    "            'labels': torch.stack(batch_labels),\n",
    "            'attention_mask': torch.stack(batch_attention_mask)\n",
    "        }\n",
    "\n",
    "# Manual forward pass function\n",
    "def manual_forward_pass(model, batch):\n",
    "    \"\"\"\n",
    "    Manually handle model forward pass and loss computation\n",
    "    \"\"\"\n",
    "    pixel_values = batch['pixel_values'].to(DEVICE)\n",
    "    input_ids = batch['input_ids'].to(DEVICE)\n",
    "    attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "    labels = batch['labels'].to(DEVICE)\n",
    "    \n",
    "    # Forward pass - note: this might need adjustment based on actual model API\n",
    "    # InternVL3-hf should accept these arguments\n",
    "    outputs = model(\n",
    "        pixel_values=pixel_values,\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        labels=labels  # Model should compute loss automatically\n",
    "    )\n",
    "    \n",
    "    return outputs.loss if hasattr(outputs, 'loss') else None\n",
    "\n",
    "def main():\n",
    "    # Load tokenizer only (no processor)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "    \n",
    "    # Ensure pad token exists\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Load model\n",
    "    model = AutoModelForImageTextToText.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    model.train()\n",
    "    \n",
    "    logger.info(\"Creating dataset\")\n",
    "    dataset = ManualImageTextDataset(json_file='created_data.json', s3_bucket=S3_BUCKET)\n",
    "    logger.info(f\"Created dataset with {len(dataset)} samples\")\n",
    "    \n",
    "    logger.info(\"Creating collator\")\n",
    "    collator = ManualCollator(tokenizer, max_length=256)\n",
    "    \n",
    "    logger.info(\"Creating dataloader\")\n",
    "    dataloader = DataLoader(dataset, batch_size=8, collate_fn=collator, shuffle=True)\n",
    "    \n",
    "    # Test single batch first\n",
    "    # logger.info(\"Testing single batch...\")\n",
    "    # try:\n",
    "    #     sample_batch = next(iter(dataloader))\n",
    "    #     logger.info(\"✅ Batch creation successful!\")\n",
    "    #     logger.info(f\"Batch keys: {sample_batch.keys()}\")\n",
    "    #     logger.info(f\"Pixel values shape: {sample_batch['pixel_values'].shape}\")\n",
    "    #     logger.info(f\"Input IDs shape: {sample_batch['input_ids'].shape}\")\n",
    "        \n",
    "    # except Exception as e:\n",
    "    #     logger.error(f\"❌ Error: {e}\")\n",
    "    #     return\n",
    "    \n",
    "    # Full training loop\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "    \n",
    "    logger.info(\"Starting training...\")\n",
    "    for epoch in range(10):  # Reduced for testing\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            try:\n",
    "                loss = manual_forward_pass(model, batch)\n",
    "                \n",
    "                if loss is not None:\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    logger.info(f\"✅ Step {step}, Loss: {loss.item():.4f}\")\n",
    "                else:\n",
    "                    logger.warning(f\"⚠️ Step {step}, No loss returned\")\n",
    "                \n",
    "                if step >= 2:  # Test just a few steps\n",
    "                    break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"❌ Step {step} failed: {e}\")\n",
    "                break\n",
    "        \n",
    "        break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77bf6aa-13bd-4be8-8885-e0f152c58c0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
